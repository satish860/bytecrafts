[
  {
    "title": "Hello world",
    "slug": "hello-world",
    "date": "1992-02-25T07:52:00.000Z",
    "content": "const{Fragment:e,jsx:t,jsxs:n}=arguments[0];function _createMdxContent(o){const r={p:\"p\",...o.components},{Callout:c}=r;return c||function(e,t){throw new Error(\"Expected \"+(t?\"component\":\"object\")+\" `\"+e+\"` to be defined: you likely forgot to import, pass, or provide it.\")}(\"Callout\",!0),n(e,{children:[t(c,{type:\"warning\",children:\"Hello I am a callout\"}),\"\\n\",t(r.p,{children:\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed non risus. Suspendisse\"})]})}return{default:function(e={}){const{wrapper:n}=e.components||{};return n?t(n,{...e,children:t(_createMdxContent,{...e})}):_createMdxContent(e)}};",
    "published": true,
    "description": "This is our first blog post. Really cool",
    "permalink": "/blog/hello-world"
  },
  {
    "title": "Google Gemma 2B on Your Laptop: A Developer's Guide",
    "slug": "google-gemma-2B-on-your-Laptop-A-Developers-Guide",
    "date": "2024-03-06T03:30:00.000Z",
    "content": "const{Fragment:e,jsx:n,jsxs:i}=arguments[0];function _createMdxContent(o){const r={a:\"a\",h2:\"h2\",img:\"img\",li:\"li\",ol:\"ol\",p:\"p\",strong:\"strong\",ul:\"ul\",...o.components};return i(e,{children:[n(r.p,{children:\"As developers, we're always on the lookout for powerful, accessible AI models that we can run locally. Google's Gemma 2B has emerged as an exciting option, offering a balance of performance and efficiency. In this guide, we'll walk you through the process of getting Gemma 2B up and running on your laptop using Jan AI, an impressive open-source platform that's changing the game for local AI deployment.\"}),\"\\n\",n(r.h2,{id:\"why-jan-ai\",children:n(r.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#why-jan-ai\",children:\"Why Jan AI?\"})}),\"\\n\",n(r.p,{children:\"Before we dive into the setup, let's talk about why we're using Jan AI. As developers, we appreciate tools that are versatile, efficient, and easy to use. Jan AI ticks all these boxes:\"}),\"\\n\",i(r.ol,{children:[\"\\n\",i(r.li,{children:[n(r.strong,{children:\"100% Offline Operation\"}),\": Run your AI models without internet connectivity – perfect for sensitive data or offline environments.\"]}),\"\\n\",i(r.li,{children:[n(r.strong,{children:\"Universal Architecture Support\"}),\": From NVIDIA GPUs to Apple M-series chips, Jan AI has you covered.\"]}),\"\\n\",i(r.li,{children:[n(r.strong,{children:\"User-Friendly Interface\"}),\": Simplifies the process of managing and running various AI models.\"]}),\"\\n\",i(r.li,{children:[n(r.strong,{children:\"Open-Source\"}),\": Benefit from community contributions and the ability to customize as needed.\"]}),\"\\n\"]}),\"\\n\",n(r.p,{children:n(r.img,{src:\"https://github.com/janhq/jan/raw/dev/demo.gif\",alt:\"Jan AI Demo\"})}),\"\\n\",n(r.h2,{id:\"step-1-setting-up-jan-ai\",children:n(r.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#step-1-setting-up-jan-ai\",children:\"Step 1: Setting Up Jan AI\"})}),\"\\n\",n(r.p,{children:\"Getting started with Jan AI is straightforward:\"}),\"\\n\",i(r.ol,{children:[\"\\n\",i(r.li,{children:[\"Head over to \",n(r.a,{href:\"https://jan.ai/\",children:\"https://jan.ai/\"}),\".\"]}),\"\\n\",n(r.li,{children:\"Download the appropriate package for your operating system.\"}),\"\\n\",n(r.li,{children:\"Follow the installation prompts to get Jan AI set up on your machine.\"}),\"\\n\"]}),\"\\n\",n(r.p,{children:\"[Developer Note: Consider adding a screenshot of the Jan AI download page here]\"}),\"\\n\",n(r.h2,{id:\"step-2-downloading-gemma-2b\",children:n(r.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#step-2-downloading-gemma-2b\",children:\"Step 2: Downloading Gemma 2B\"})}),\"\\n\",n(r.p,{children:\"Once Jan AI is installed, it's time to get our hands on Gemma 2B:\"}),\"\\n\",i(r.ol,{children:[\"\\n\",i(r.li,{children:[\"\\n\",n(r.p,{children:'Launch Jan AI and navigate to the \"Explore\" section.'}),\"\\n\"]}),\"\\n\",i(r.li,{children:[\"\\n\",n(r.p,{children:'Use the search function to find \"Gemma 2B\".'}),\"\\n\"]}),\"\\n\",i(r.li,{children:[\"\\n\",n(r.p,{children:\"Before you hit that download button, take note of the system compatibility information Jan AI provides:\"}),\"\\n\",i(r.ul,{children:[\"\\n\",i(r.li,{children:[n(r.strong,{children:\"File Size\"}),\": Crucial for managing your storage.\"]}),\"\\n\",i(r.li,{children:[n(r.strong,{children:\"Performance Indicators\"}),\": Jan AI will let you know if the model might run slowly on your device.\"]}),\"\\n\",i(r.li,{children:[n(r.strong,{children:\"RAM Requirements\"}),\": Ensure your system has enough memory to handle the model.\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\",n(r.p,{children:n(r.img,{src:\"/static/Screenshot-28a3cc2c.jpg\",alt:\"Jan AI Model Compatibility Screenshot\"})}),\"\\n\",i(r.ol,{start:\"4\",children:[\"\\n\",n(r.li,{children:\"If your system meets the requirements, go ahead and download Gemma 2B.\"}),\"\\n\"]}),\"\\n\",n(r.h2,{id:\"step-3-running-gemma-2b\",children:n(r.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#step-3-running-gemma-2b\",children:\"Step 3: Running Gemma 2B\"})}),\"\\n\",n(r.p,{children:\"Now for the moment of truth – running Gemma 2B:\"}),\"\\n\",i(r.ol,{children:[\"\\n\",n(r.li,{children:'In Jan AI, click on the \"Chat\" section.'}),\"\\n\",n(r.li,{children:\"Look for the model selection dropdown.\"}),\"\\n\",n(r.li,{children:'Choose \"Gemma 2B\" from the list.'}),\"\\n\",n(r.li,{children:\"Start chatting! Your local instance of Gemma 2B is now ready to process your prompts.\"}),\"\\n\"]}),\"\\n\",n(r.h2,{id:\"developer-insights\",children:n(r.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#developer-insights\",children:\"Developer Insights\"})}),\"\\n\",n(r.p,{children:\"As you start working with Gemma 2B, keep these tips in mind:\"}),\"\\n\",i(r.ol,{children:[\"\\n\",i(r.li,{children:[\"\\n\",i(r.p,{children:[n(r.strong,{children:\"Prompt Engineering\"}),\": Gemma 2B, like all language models, responds best to clear, well-structured prompts. Experiment with different phrasings to optimize your results.\"]}),\"\\n\"]}),\"\\n\",i(r.li,{children:[\"\\n\",i(r.p,{children:[n(r.strong,{children:\"Performance Tuning\"}),\": If you're running on a less powerful machine, consider tweaking Jan AI's settings for better performance. Look for options to adjust the model's precision or inference settings.\"]}),\"\\n\"]}),\"\\n\",i(r.li,{children:[\"\\n\",i(r.p,{children:[n(r.strong,{children:\"Offline Capabilities\"}),\": Leverage Gemma 2B's offline nature for projects involving sensitive data or for developing apps that need to function without internet connectivity.\"]}),\"\\n\"]}),\"\\n\",i(r.li,{children:[\"\\n\",i(r.p,{children:[n(r.strong,{children:\"API Integration\"}),\": While we've focused on the chat interface, explore Jan AI's API options to integrate Gemma 2B into your applications programmatically.\"]}),\"\\n\"]}),\"\\n\",i(r.li,{children:[\"\\n\",i(r.p,{children:[n(r.strong,{children:\"Model Comparison\"}),\": Jan AI makes it easy to switch between models. Consider comparing Gemma 2B's performance with other models for your specific use cases.\"]}),\"\\n\"]}),\"\\n\"]}),\"\\n\",n(r.h2,{id:\"conclusion\",children:n(r.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#conclusion\",children:\"Conclusion\"})}),\"\\n\",n(r.p,{children:\"Running Google Gemma 2B locally with Jan AI opens up a world of possibilities for developers. Whether you're building the next revolutionary chatbot, working on NLP projects, or simply exploring the capabilities of AI, this setup provides a powerful, flexible foundation.\"}),\"\\n\",n(r.p,{children:\"Remember, the field of AI is rapidly evolving. Keep an eye on Jan AI's updates and the broader AI community for new models and features that could enhance your development workflow.\"}),\"\\n\",n(r.p,{children:\"Happy coding, and enjoy exploring the capabilities of Gemma 2B right on your laptop!\"})]})}return{default:function(e={}){const{wrapper:i}=e.components||{};return i?n(i,{...e,children:n(_createMdxContent,{...e})}):_createMdxContent(e)}};",
    "published": true,
    "description": "Learn how to run Google's Gemma 2B AI model locally on your laptop using Jan AI, opening up new possibilities for offline AI development and experimentation.",
    "permalink": "/blog/google-gemma-2B-on-your-Laptop-A-Developers-Guide"
  },
  {
    "title": "Build Your Own Agent Framework - Step by Step",
    "slug": "build-your-own-agent-framework-step-by-step",
    "date": "2024-11-14T03:30:00.000Z",
    "content": "const{Fragment:e,jsx:n,jsxs:t}=arguments[0];function _createMdxContent(r){const o={a:\"a\",blockquote:\"blockquote\",code:\"code\",h1:\"h1\",h3:\"h3\",h4:\"h4\",p:\"p\",pre:\"pre\",strong:\"strong\",...r.components};return t(e,{children:[n(o.p,{children:\"Best way to learn is by doing. So lets our build our own Agent Framework like OpenAI Swarm. Well we need to start small.\"}),\"\\n\",t(o.blockquote,{children:[\"\\n\",n(o.h4,{id:\"pre-requisites\",children:n(o.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#pre-requisites\",children:\"Pre-requisites\"})}),\"\\n\"]}),\"\\n\",t(o.blockquote,{children:[\"\\n\",n(o.h4,{id:\"i-assume-you-have-some-knowledge-of-python-and-used-a-openai-api-if-not-you-can-follow-the-getting-started-guide\",children:t(o.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#i-assume-you-have-some-knowledge-of-python-and-used-a-openai-api-if-not-you-can-follow-the-getting-started-guide\",children:[\"I assume you have some knowledge of Python and used a OpenAI API. If not you can follow the \",n(o.a,{href:\"https://platform.openai.com/docs/api-reference/introduction\",children:\"Getting Started Guide\"})]})}),\"\\n\"]}),\"\\n\",n(o.p,{children:\"Now that the pre-requisites are out of the way, we can start building our own Agent Framework.\"}),\"\\n\",n(o.h3,{id:\"so-what-is-a-agent-\",children:n(o.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#so-what-is-a-agent-\",children:\"So What is a Agent ?\"})}),\"\\n\",n(o.p,{children:\"My own take is the Agents are something which can take an input, perform some task and return a output. Simple as that. If you want to get the latest news from the web, LLM doesnt have the capability to do that. But a agent can browse the web, read the news and then tell you the latest news.\"}),\"\\n\",t(o.blockquote,{children:[\"\\n\",n(o.p,{children:\"Yes there is many other definitions of Agent, but for now lets stick with my definition for making a framework. We will talk about memory and other things in later posts.\"}),\"\\n\"]}),\"\\n\",n(o.p,{children:\"So how can you build a Agent. Well there are many ways to do that. But the most popular way is to use LLMs with function calling capabilities.\"}),\"\\n\",n(o.h1,{id:\"function-calling\",children:n(o.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#function-calling\",children:\"Function Calling\"})}),\"\\n\",n(o.p,{children:\"LLMs a Passive responders, You ask a question and they respond. But with function calling capabilities, you can ask it to respond if it knows the answer else ask it to use the tools like web search, database lookup, etc. Then it can use the tools to get the answer and then return the answer.\"}),\"\\n\",n(o.p,{children:\"Rather than explaining the theory, lets jump into the code.\"}),\"\\n\",n(o.pre,{children:n(o.code,{children:\"! pip install openai \\n\"})}),\"\\n\",t(o.p,{children:[\"Now we need to setup the OpenAI API key. You can get your API key from \",n(o.a,{href:\"https://platform.openai.com/api-keys\",children:\"here\"})]}),\"\\n\",n(o.pre,{children:n(o.code,{children:'import openai\\r\\n\\r\\nopenai.api_key = \"sk-proj-your-openai-api-key\"\\n'})}),\"\\n\",t(o.p,{children:[\"Now we need to create a function calling agent with defining a simple function which can search the web. You can use any API to search the web. I am using \",n(o.a,{href:\"https://ydc-index.io/\",children:\"YDC-Index\"}),\" for this example. Its You.com search API.\"]}),\"\\n\",n(o.pre,{children:n(o.code,{children:'import requests\\r\\n\\r\\ndef get_ai_snippets_for_query(query):\\r\\n    headers = {\"X-API-Key\": \"your-api-key\"}\\r\\n    params = {\"query\": query}\\r\\n    return requests.get(\\r\\n        f\"https://api.ydc-index.io/search?query={query}\",\\r\\n        params=params,\\r\\n        headers=headers,\\r\\n    ).json()\\r\\nresults = get_ai_snippets_for_query(\"What is the latest news in india\")\\r\\nprint(results)\\r\\n\\n'})}),\"\\n\",n(o.p,{children:\"Now we need to introduce this function to LLM. But how can we do that, Well you need to convert the function into a string and then pass it to LLM. So let me define a function to convert the function into a string.\"}),\"\\n\",n(o.pre,{children:n(o.code,{children:'import inspect\\r\\n\\r\\ndef function_to_json(func) -> dict:\\r\\n    \"\"\"\\r\\n    Converts a Python function into a JSON-serializable dictionary\\r\\n    that describes the function\\'s signature, including its name,\\r\\n    description, and parameters.\\r\\n\\r\\n    Args:\\r\\n        func: The function to be converted.\\r\\n\\r\\n    Returns:\\r\\n        A dictionary representing the function\\'s signature in JSON format.\\r\\n    \"\"\"\\r\\n    type_map = {\\r\\n        str: \"string\",\\r\\n        int: \"integer\",\\r\\n        float: \"number\",\\r\\n        bool: \"boolean\",\\r\\n        list: \"array\",\\r\\n        dict: \"object\",\\r\\n        type(None): \"null\",\\r\\n    }\\r\\n\\r\\n    try:\\r\\n        signature = inspect.signature(func)\\r\\n    except ValueError as e:\\r\\n        raise ValueError(\\r\\n            f\"Failed to get signature for function {func.__name__}: {str(e)}\"\\r\\n        )\\r\\n\\r\\n    parameters = {}\\r\\n    for param in signature.parameters.values():\\r\\n        try:\\r\\n            param_type = type_map.get(param.annotation, \"string\")\\r\\n        except KeyError as e:\\r\\n            raise KeyError(\\r\\n                f\"Unknown type annotation {param.annotation} for parameter {param.name}: {str(e)}\"\\r\\n            )\\r\\n        parameters[param.name] = {\"type\": param_type}\\r\\n\\r\\n    required = [\\r\\n        param.name\\r\\n        for param in signature.parameters.values()\\r\\n        if param.default == inspect._empty\\r\\n    ]\\r\\n\\r\\n    return {\\r\\n        \"type\": \"function\",\\r\\n        \"function\": {\\r\\n            \"name\": func.__name__,\\r\\n            \"description\": func.__doc__ or \"\",\\r\\n            \"parameters\": {\\r\\n                \"type\": \"object\",\\r\\n                \"properties\": parameters,\\r\\n                \"required\": required,\\r\\n            },\\r\\n        },\\r\\n    }\\n'})}),\"\\n\",n(o.p,{children:\"This utility can take any arbitrary function and convert it into a JSON Schema which can be used by LLM to call the function.\"}),\"\\n\",n(o.p,{children:\"Now we need to call this function to LLM.\"}),\"\\n\",n(o.pre,{children:n(o.code,{children:\"websearch_tool = function_to_json(get_ai_snippets_for_query)\\n\"})}),\"\\n\",n(o.p,{children:\"Now we need to create a system prompt and then call the LLM with the history and the tools.\"}),\"\\n\",n(o.pre,{children:n(o.code,{children:'import json\\r\\n\\r\\n\\r\\nSYSTEM_PROMPT = \"\"\"\\r\\nYou are a helpful assistant that can answer questions and help with tasks. If you need any information from the internet, you can use the tools provided to you.\\r\\n\"\"\"\\r\\n\\r\\nhistory = []\\r\\nhistory.append({\"role\": \"system\", \"content\": SYSTEM_PROMPT})\\r\\nhistory.append({\"role\": \"user\", \"content\": \"What is the Latest news about Supreme court of india?\"})\\r\\n\\r\\nresponse = openai.chat.completions.create(\\r\\n    model=\"gpt-4o-mini\",\\r\\n    messages=history,\\r\\n    tools=[websearch_tool]\\r\\n)\\r\\n\\r\\nmessage = response.choices[0].message\\r\\n\\r\\nprint(message.model_dump_json())\\r\\nhistory.append(json.loads(message.model_dump_json()))\\n'})}),\"\\n\",n(o.p,{children:n(o.strong,{children:\"Output:\"})}),\"\\n\",t(o.p,{children:[\"Note the \",n(o.code,{children:\"tool_calls\"}),\" key in the response. This is the tool call which we need to use to call the function.\"]}),\"\\n\",n(o.pre,{children:n(o.code,{children:'{\"content\":null,\"refusal\":null,\"role\":\"assistant\",\"audio\":null,\"function_call\":null,\"tool_calls\":[{\"id\":\"call_TQQ8Md20X9BJvSywhLvlAUTB\",\"function\":{\"arguments\":\"{\\\\\"query\\\\\":\\\\\"Latest news Supreme Court of India\\\\\"}\",\"name\":\"get_ai_snippets_for_query\"},\"type\":\"function\"}]}\\n'})}),\"\\n\",t(o.p,{children:[\"It contains the \",n(o.code,{children:\"id\"}),\" of the tool call and the \",n(o.code,{children:\"function\"}),\" which needs to be called. In this case I asked the LLM to provide a Latest news which in turn will make the LLM think and then call the \",n(o.code,{children:\"get_ai_snippets_for_query\"}),\" function with the query as \",n(o.code,{children:\"Latest news Supreme Court of India\"}),\".\"]}),\"\\n\",n(o.p,{children:\"Now we need to call the function with the tool call id and the arguments.\"}),\"\\n\",n(o.pre,{children:n(o.code,{children:'for tools in message.tool_calls:\\r\\n    args = json.loads(tools.function.arguments)\\r\\n    raw_result = get_ai_snippets_for_query(**args)\\r\\n    history.append({\"role\": \"tool\", \"tool_call_id\": tools.id,\"tool_name\":tools.function.name, \"content\": raw_result})\\r\\n\\n'})}),\"\\n\",n(o.p,{children:\"Now that we have the result from the tool, we need to append it to the history and then call the LLM again to get the final answer.\"}),\"\\n\",n(o.pre,{children:n(o.code,{children:'function_response = openai.chat.completions.create(\\r\\n    model=\"gpt-4o-mini\",\\r\\n    messages=history,\\r\\n)\\r\\n\\r\\nprint(function_response.choices[0].message.content)\\r\\n\\n'})}),\"\\n\",n(o.p,{children:n(o.strong,{children:\"Output:\"})}),\"\\n\",n(o.pre,{children:n(o.code,{children:\"Here are some of the latest developments and news regarding the Supreme Court of India:\\r\\n\\r\\n1. **Child Pornography Ruling**: The Supreme Court ruled that merely possessing child pornographic material constitutes a criminal offence. This decision underscores legal culpability for preparatory actions related to inchoate crimes.\\r\\n\\r\\n2. **Caste-Based Discrimination Ban**: A significant verdict from the Supreme Court has banned caste-based discrimination, specifically prohibiting practices such as the division of manual labor and segregation of prisoners belonging to de-notified tribes.\\r\\n\\r\\n3. **YouTube Channel Hack**: The Supreme Court's official YouTube channel was compromised, displaying unauthorized content instead of the usual court hearing live streams. YouTube has since acted to remove the hacked content.\\r\\n\\r\\n4. **Maternity Benefits Inquiry**: The Supreme Court has questioned why maternity benefits are only available to adoptive mothers if the adopted child is under three months old, seeking clarity from the Centre on this policy.\\r\\n\\r\\n5. **Stakeholders Consultation on Disabilities**: The Supreme Court announced a two-day National Annual Stakeholders Consultation focusing on the rights of children living with disabilities, set to take place on September 28-29, 2024.\\r\\n\\r\\n6. **Emissions Standards Compliance**: The Air Quality Panel has indicated to the Supreme Court that states need to comply with emissions standards in order to address pollution issues effectively.\\r\\n\\r\\nThese updates highlight ongoing legal and social issues being addressed by the Supreme Court, reflecting its role in shaping human rights and governance in India.\\r\\n\\n\"})}),\"\\n\",n(o.p,{children:\"Well that was simple. We just created a simple agent which can search the web and return the result. But this is just the beginning. We can add more tools and make it more powerful. We can also add memory to it so that it can remember the previous interactions.\"}),\"\\n\",n(o.p,{children:\"In the next post, we will create a another abstraction on top of this like a Agent calling a Another Agent which is called a Agent.\"})]})}return{default:function(e={}){const{wrapper:t}=e.components||{};return t?n(t,{...e,children:n(_createMdxContent,{...e})}):_createMdxContent(e)}};",
    "published": true,
    "description": "A step by step guide to building your own Agent Framework.",
    "permalink": "/blog/build-your-own-agent-framework-step-by-step"
  }
]